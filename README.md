# ğŸ­ Sentiment Analyzer (Mistral)

A modern AI-powered sentiment analysis application that uses the **Mistral language model** via Ollama to classify text sentiment as Positive, Negative, or Neutral.

![Python](https://img.shields.io/badge/python-v3.8+-blue.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-005571?style=flat&logo=fastapi)
![Streamlit](https://img.shields.io/badge/Streamlit-FF4B4B?style=flat&logo=Streamlit&logoColor=white)
![Mistral AI](https://img.shields.io/badge/Mistral_AI-FF7000?style=flat)

## âœ¨ Features

- **ğŸš€ Fast Analysis**: Real-time sentiment classification using Mistral AI
- **ğŸ¨ Modern UI**: Beautiful Streamlit frontend with custom styling
- **ğŸ”§ Robust Backend**: FastAPI backend with error handling and health checks
- **ğŸ“Š Detailed Results**: Processing time, confidence scores, and analysis details
- **ğŸ’¡ Sample Texts**: Built-in examples to test different sentiment types
- **ğŸ” System Monitoring**: Health check dashboard for system status

## ğŸ“ Project Files

```
sentiment-analyzer-mistral/
â”‚
â”œâ”€â”€ backend/
â”‚   â””â”€â”€ main.py              # FastAPI backend server
â”‚
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ app.py              # Streamlit frontend application
â”‚
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ test_setup.py           # Setup verification script
â”œâ”€â”€ start_app.py            # Cross-platform startup script
â”œâ”€â”€ start_app.bat           # Windows batch file (Windows only)
â”œâ”€â”€ README.md               # Documentation
â””â”€â”€ .gitignore             # Git ignore rules
```
## ğŸš€ Quick Start

### Prerequisites

- Python 3.8 or higher
- [Ollama](https://ollama.ai/) installed and running
- Git (for version control)

### Installation & Setup

1. **Create project directory and files**
   ```bash
   mkdir sentiment-analyzer-mistral
   cd sentiment-analyzer-mistral
   mkdir backend frontend
   ```

2. **Create all the project files** (copy the code from the artifacts above):
   - `backend/main.py` (FastAPI Backend)
   - `frontend/app.py` (Streamlit Frontend) 
   - `requirements.txt`
   - `test_setup.py` (Test script)
   - `start_app.py` (Startup script)

3. **Set up Python environment**
   ```bash
   python -m venv venv
   
   # On Windows
   venv\Scripts\activate
   
   # On macOS/Linux
   source venv/bin/activate
   
   pip install -r requirements.txt
   ```

4. **Set up Ollama and Mistral**
   ```bash
   # Make sure Ollama is running
   ollama serve
   
   # Pull the Mistral model
   ollama pull mistral
   ```

5. **Test your setup**
   ```bash
   python test_setup.py
   ```

6. **Run the application**
   
   **Option A: Windows Batch File (Windows Only)**
   ```cmd
   start_app.bat
   ```
   
   **Option B: Python Startup Script (Cross-Platform)**
   ```bash
   python start_app.py
   ```
   
   **Option C: Manual startup (Two terminals)**
   ```bash
   # Terminal 1 - Backend
   uvicorn backend.main:app --reload
   
   # Terminal 2 - Frontend  
   streamlit run frontend/app.py
   ```

7. **Access the application**
   - Frontend: http://localhost:8501
   - Backend API: http://localhost:8000/docs

## ğŸ¯ Usage

1. **Enter Text**: Type or paste your text in the input area
2. **Analyze**: Click the "Analyze Sentiment" button
3. **View Results**: Get instant sentiment classification with detailed analysis
4. **Try Samples**: Use built-in sample texts to test different sentiment types

### Example Inputs

- **Positive**: "I absolutely love this new product! It works perfectly."
- **Negative**: "This service is terrible and I'm very disappointed."
- **Neutral**: "The meeting is scheduled for 2 PM tomorrow in room B."

## ğŸ”§ API Endpoints

### POST `/analyze/`
Analyze the sentiment of provided text.

**Request Body:**
```json
{
  "text": "Your text here"
}
```

**Response:**
```json
{
  "sentiment": "Positive",
  "text": "Your text here",
  "raw_response": "Positive"
}
```

### GET `/health/`
Check system health and Ollama connection status.

**Response:**
```json
{
  "api_status": "healthy",
  "ollama_status": "connected",
  "mistral_available": true
}
```

## ğŸ› Troubleshooting

### Common Issues

1. **"Cannot connect to Ollama service"**
   - Ensure Ollama is running: `ollama serve`
   - Check if Mistral model is available: `ollama list`
   - Pull the model if missing: `ollama pull mistral`

2. **"Cannot connect to backend API"**
   - Verify the FastAPI server is running on port 8000
   - Check firewall settings
   - Try: `curl http://localhost:8000/health/`

3. **Streamlit connection errors**
   - Ensure backend is running first
   - Check if port 8501 is available
   - Try refreshing the browser page

### Performance Tips

- For faster responses, keep Ollama running in the background
- The first analysis may take longer as the model loads
- Consider using a GPU for better performance with larger texts

## ğŸ› ï¸ Development

### Project Structure

- `backend/main.py`: FastAPI application with sentiment analysis endpoints
- `frontend/app.py`: Streamlit web interface with custom styling
- `requirements.txt`: Python package dependencies

### Running in Development Mode

```bash
# Backend with auto-reload
uvicorn backend.main:app --reload

# Frontend with auto-reload (automatic)
streamlit run frontend/app.py
```

### Adding Features

1. **Custom Models**: Modify the model name in `backend/main.py`
2. **Additional Analysis**: Add new endpoints for different NLP tasks
3. **UI Improvements**: Customize CSS in `frontend/app.py`

## ğŸ“¦ Deployment

### Docker Deployment (Optional)

Create a `Dockerfile`:

```dockerfile
FROM python:3.9-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

EXPOSE 8000 8501

# You'll need to run Ollama separately
CMD ["sh", "-c", "uvicorn backend.main:app --host 0.0.0.0 --port 8000 & streamlit run frontend/app.py --server.port 8501"]
```

### Cloud Deployment

For cloud deployment, you'll need to:
1. Set up Ollama on a server with sufficient resources
2. Configure proper networking and security
3. Use environment variables for configuration
4. Consider using container orchestration (Kubernetes, Docker Swarm)

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [Mistral AI](https://mistral.ai/) for the language model
- [Ollama](https://ollama.ai/) for local model hosting
- [FastAPI](https://fastapi.tiangolo.com/) for the backend framework
- [Streamlit](https://streamlit.io/) for the frontend framework

## ğŸ“ Support

If you encounter any issues or have questions:

1. Check the [troubleshooting section](#-troubleshooting)
2. Open an issue on GitHub
3. Review the API documentation at http://localhost:8000/docs

## ğŸ“ Screenshots
![Description](Screenshot%202025-08-10%20154357.png)
---


**Happy Analyzing! ğŸ­âœ¨**



